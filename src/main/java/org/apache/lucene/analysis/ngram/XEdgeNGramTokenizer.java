begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_package
DECL|package|org.apache.lucene.analysis.ngram
package|package
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|ngram
package|;
end_package

begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one or more  * contributor license agreements.  See the NOTICE file distributed with  * this work for additional information regarding copyright ownership.  * The ASF licenses this file to You under the Apache License, Version 2.0  * (the "License"); you may not use this file except in compliance with  * the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|common
operator|.
name|lucene
operator|.
name|Lucene
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Reader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|Tokenizer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|Version
import|;
end_import

begin_comment
comment|/**  * Tokenizes the input from an edge into n-grams of given size(s).  *<p>  * This {@link Tokenizer} create n-grams from the beginning edge or ending edge of a input token.  *<p><a name="version" /> As of Lucene 4.4, this tokenizer<ul>  *<li>can handle<code>maxGram</code> larger than 1024 chars, but beware that this will result in increased memory usage  *<li>doesn't trim the input,  *<li>sets position increments equal to 1 instead of 1 for the first token and 0 for all other ones  *<li>doesn't support backward n-grams anymore.  *<li>supports {@link #isTokenChar(int) pre-tokenization},  *<li>correctly handles supplementary characters.  *</ul>  *<p>Although<b style="color:red">highly</b> discouraged, it is still possible  * to use the old behavior through {@link Lucene43XEdgeXNGramTokenizer}.  */
end_comment

begin_class
DECL|class|XEdgeNGramTokenizer
specifier|public
class|class
name|XEdgeNGramTokenizer
extends|extends
name|XNGramTokenizer
block|{
static|static
block|{
comment|// LUCENE MONITOR: this should be in Lucene 4.4 copied from Revision: 1492640.
assert|assert
name|Lucene
operator|.
name|VERSION
operator|==
name|Version
operator|.
name|LUCENE_43
operator|:
literal|"Elasticsearch has upgraded to Lucene Version: ["
operator|+
name|Lucene
operator|.
name|VERSION
operator|+
literal|"] this class should be removed"
assert|;
block|}
DECL|field|DEFAULT_MAX_GRAM_SIZE
specifier|public
specifier|static
specifier|final
name|int
name|DEFAULT_MAX_GRAM_SIZE
init|=
literal|1
decl_stmt|;
DECL|field|DEFAULT_MIN_GRAM_SIZE
specifier|public
specifier|static
specifier|final
name|int
name|DEFAULT_MIN_GRAM_SIZE
init|=
literal|1
decl_stmt|;
comment|/**    * Creates XEdgeXNGramTokenizer that can generate n-grams in the sizes of the given range    *    * @param version the<a href="#version">Lucene match version</a>    * @param input {@link Reader} holding the input to be tokenized    * @param minGram the smallest n-gram to generate    * @param maxGram the largest n-gram to generate    */
DECL|method|XEdgeNGramTokenizer
specifier|public
name|XEdgeNGramTokenizer
parameter_list|(
name|Version
name|version
parameter_list|,
name|Reader
name|input
parameter_list|,
name|int
name|minGram
parameter_list|,
name|int
name|maxGram
parameter_list|)
block|{
name|super
argument_list|(
name|version
argument_list|,
name|input
argument_list|,
name|minGram
argument_list|,
name|maxGram
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
comment|/**    * Creates XEdgeXNGramTokenizer that can generate n-grams in the sizes of the given range    *    * @param version the<a href="#version">Lucene match version</a>    * @param factory {@link org.apache.lucene.util.AttributeSource.AttributeFactory} to use    * @param input {@link Reader} holding the input to be tokenized    * @param minGram the smallest n-gram to generate    * @param maxGram the largest n-gram to generate    */
DECL|method|XEdgeNGramTokenizer
specifier|public
name|XEdgeNGramTokenizer
parameter_list|(
name|Version
name|version
parameter_list|,
name|AttributeFactory
name|factory
parameter_list|,
name|Reader
name|input
parameter_list|,
name|int
name|minGram
parameter_list|,
name|int
name|maxGram
parameter_list|)
block|{
name|super
argument_list|(
name|version
argument_list|,
name|factory
argument_list|,
name|input
argument_list|,
name|minGram
argument_list|,
name|maxGram
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
end_class

end_unit

