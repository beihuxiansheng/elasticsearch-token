begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to Elasticsearch under one or more contributor  * license agreements. See the NOTICE file distributed with  * this work for additional information regarding copyright  * ownership. Elasticsearch licenses this file to you under  * the Apache License, Version 2.0 (the "License"); you may  * not use this file except in compliance with the License.  * You may obtain a copy of the License at  *  *    http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing,  * software distributed under the License is distributed on an  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY  * KIND, either express or implied.  See the License for the  * specific language governing permissions and limitations  * under the License.  */
end_comment

begin_package
DECL|package|org.apache.lucene.analysis.miscellaneous
package|package
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|miscellaneous
package|;
end_package

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|FilteringTokenFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|TokenFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|TokenStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|tokenattributes
operator|.
name|TermToBytesRefAttribute
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|util
operator|.
name|BytesRef
import|;
end_import

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|common
operator|.
name|hash
operator|.
name|MurmurHash3
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_comment
comment|/**  * Inspects token streams for duplicate sequences of tokens. Token sequences  * have a minimum length - 6 is a good heuristic as it avoids filtering common  * idioms/phrases but detects longer sections that are typical of cut+paste  * copies of text.  *   *<p>  * Internally each token is hashed/moduloed into a single byte (so 256 possible  * values for each token) and then recorded in a trie of seen byte sequences  * using a {@link DuplicateByteSequenceSpotter}. This trie is passed into the  * TokenFilter constructor so a single object can be reused across multiple  * documents.  *   *<p>  * The emitDuplicates setting controls if duplicate tokens are filtered from  * results or are output (the {@link DuplicateSequenceAttribute} attribute can  * be used to inspect the number of prior sightings when emitDuplicates is true)  */
end_comment

begin_class
DECL|class|DeDuplicatingTokenFilter
specifier|public
class|class
name|DeDuplicatingTokenFilter
extends|extends
name|FilteringTokenFilter
block|{
DECL|field|seqAtt
specifier|private
specifier|final
name|DuplicateSequenceAttribute
name|seqAtt
init|=
name|addAttribute
argument_list|(
name|DuplicateSequenceAttribute
operator|.
name|class
argument_list|)
decl_stmt|;
DECL|field|emitDuplicates
specifier|private
specifier|final
name|boolean
name|emitDuplicates
decl_stmt|;
DECL|field|seed
specifier|static
specifier|final
name|MurmurHash3
operator|.
name|Hash128
name|seed
init|=
operator|new
name|MurmurHash3
operator|.
name|Hash128
argument_list|()
decl_stmt|;
DECL|method|DeDuplicatingTokenFilter
specifier|public
name|DeDuplicatingTokenFilter
parameter_list|(
name|TokenStream
name|in
parameter_list|,
name|DuplicateByteSequenceSpotter
name|byteStreamDuplicateSpotter
parameter_list|)
block|{
name|this
argument_list|(
name|in
argument_list|,
name|byteStreamDuplicateSpotter
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**      *       * @param in      *            The input token stream      * @param byteStreamDuplicateSpotter      *            object which retains trie of token sequences      * @param emitDuplicates      *            true if duplicate tokens are to be emitted (use      *            {@link DuplicateSequenceAttribute} attribute to inspect number      *            of prior sightings of tokens as part of a sequence).      */
DECL|method|DeDuplicatingTokenFilter
specifier|public
name|DeDuplicatingTokenFilter
parameter_list|(
name|TokenStream
name|in
parameter_list|,
name|DuplicateByteSequenceSpotter
name|byteStreamDuplicateSpotter
parameter_list|,
name|boolean
name|emitDuplicates
parameter_list|)
block|{
name|super
argument_list|(
operator|new
name|DuplicateTaggingFilter
argument_list|(
name|byteStreamDuplicateSpotter
argument_list|,
name|in
argument_list|)
argument_list|)
expr_stmt|;
name|this
operator|.
name|emitDuplicates
operator|=
name|emitDuplicates
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|accept
specifier|protected
name|boolean
name|accept
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|emitDuplicates
operator|||
name|seqAtt
operator|.
name|getNumPriorUsesInASequence
argument_list|()
operator|<
literal|1
return|;
block|}
DECL|class|DuplicateTaggingFilter
specifier|private
specifier|static
class|class
name|DuplicateTaggingFilter
extends|extends
name|TokenFilter
block|{
DECL|field|seqAtt
specifier|private
specifier|final
name|DuplicateSequenceAttribute
name|seqAtt
init|=
name|addAttribute
argument_list|(
name|DuplicateSequenceAttribute
operator|.
name|class
argument_list|)
decl_stmt|;
DECL|field|termBytesAtt
name|TermToBytesRefAttribute
name|termBytesAtt
init|=
name|addAttribute
argument_list|(
name|TermToBytesRefAttribute
operator|.
name|class
argument_list|)
decl_stmt|;
DECL|field|byteStreamDuplicateSpotter
specifier|private
name|DuplicateByteSequenceSpotter
name|byteStreamDuplicateSpotter
decl_stmt|;
DECL|field|allTokens
specifier|private
name|ArrayList
argument_list|<
name|State
argument_list|>
name|allTokens
decl_stmt|;
DECL|field|pos
name|int
name|pos
init|=
literal|0
decl_stmt|;
DECL|field|windowSize
specifier|private
specifier|final
name|int
name|windowSize
decl_stmt|;
DECL|method|DuplicateTaggingFilter
specifier|protected
name|DuplicateTaggingFilter
parameter_list|(
name|DuplicateByteSequenceSpotter
name|byteStreamDuplicateSpotter
parameter_list|,
name|TokenStream
name|input
parameter_list|)
block|{
name|super
argument_list|(
name|input
argument_list|)
expr_stmt|;
name|this
operator|.
name|byteStreamDuplicateSpotter
operator|=
name|byteStreamDuplicateSpotter
expr_stmt|;
name|this
operator|.
name|windowSize
operator|=
name|DuplicateByteSequenceSpotter
operator|.
name|TREE_DEPTH
expr_stmt|;
block|}
annotation|@
name|Override
DECL|method|incrementToken
specifier|public
specifier|final
name|boolean
name|incrementToken
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|allTokens
operator|==
literal|null
condition|)
block|{
name|loadAllTokens
argument_list|()
expr_stmt|;
block|}
name|clearAttributes
argument_list|()
expr_stmt|;
if|if
condition|(
name|pos
operator|<
name|allTokens
operator|.
name|size
argument_list|()
condition|)
block|{
name|State
name|earlierToken
init|=
name|allTokens
operator|.
name|get
argument_list|(
name|pos
argument_list|)
decl_stmt|;
name|pos
operator|++
expr_stmt|;
name|restoreState
argument_list|(
name|earlierToken
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
else|else
block|{
return|return
literal|false
return|;
block|}
block|}
DECL|method|loadAllTokens
specifier|public
name|void
name|loadAllTokens
parameter_list|()
throws|throws
name|IOException
block|{
comment|// TODO consider changing this implementation to emit tokens as-we-go
comment|// rather than buffering all. However this array is perhaps not the
comment|// bulk of memory usage (in practice the dupSequenceSpotter requires
comment|// ~5x the original content size in its internal tree ).
name|allTokens
operator|=
operator|new
name|ArrayList
argument_list|<
name|State
argument_list|>
argument_list|(
literal|256
argument_list|)
expr_stmt|;
comment|/*              * Given the bytes 123456123456 and a duplicate sequence size of 6              * the byteStreamDuplicateSpotter will only flag the final byte as              * part of a duplicate sequence due to the byte-at-a-time streaming              * nature of its assessments. When this happens we retain a buffer              * of the last 6 tokens so that we can mark the states of prior              * tokens (bytes 7 to 11) as also being duplicates              */
name|pos
operator|=
literal|0
expr_stmt|;
name|boolean
name|isWrapped
init|=
literal|false
decl_stmt|;
name|State
name|priorStatesBuffer
index|[]
init|=
operator|new
name|State
index|[
name|windowSize
index|]
decl_stmt|;
name|short
name|priorMaxNumSightings
index|[]
init|=
operator|new
name|short
index|[
name|windowSize
index|]
decl_stmt|;
name|int
name|cursor
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|input
operator|.
name|incrementToken
argument_list|()
condition|)
block|{
name|BytesRef
name|bytesRef
init|=
name|termBytesAtt
operator|.
name|getBytesRef
argument_list|()
decl_stmt|;
name|long
name|tokenHash
init|=
name|MurmurHash3
operator|.
name|hash128
argument_list|(
name|bytesRef
operator|.
name|bytes
argument_list|,
name|bytesRef
operator|.
name|offset
argument_list|,
name|bytesRef
operator|.
name|length
argument_list|,
literal|0
argument_list|,
name|seed
argument_list|)
operator|.
name|h1
decl_stmt|;
name|byte
name|tokenByte
init|=
call|(
name|byte
call|)
argument_list|(
name|tokenHash
operator|&
literal|0xFF
argument_list|)
decl_stmt|;
name|short
name|numSightings
init|=
name|byteStreamDuplicateSpotter
operator|.
name|addByte
argument_list|(
name|tokenByte
argument_list|)
decl_stmt|;
name|priorStatesBuffer
index|[
name|cursor
index|]
operator|=
name|captureState
argument_list|()
expr_stmt|;
comment|// Revise prior captured State objects if the latest
comment|// token is marked as a duplicate
if|if
condition|(
name|numSightings
operator|>=
literal|1
condition|)
block|{
name|int
name|numLengthsToRecord
init|=
name|windowSize
decl_stmt|;
name|int
name|pos
init|=
name|cursor
decl_stmt|;
while|while
condition|(
name|numLengthsToRecord
operator|>
literal|0
condition|)
block|{
if|if
condition|(
name|pos
operator|<
literal|0
condition|)
block|{
name|pos
operator|=
name|windowSize
operator|-
literal|1
expr_stmt|;
block|}
name|priorMaxNumSightings
index|[
name|pos
index|]
operator|=
operator|(
name|short
operator|)
name|Math
operator|.
name|max
argument_list|(
name|priorMaxNumSightings
index|[
name|pos
index|]
argument_list|,
name|numSightings
argument_list|)
expr_stmt|;
name|numLengthsToRecord
operator|--
expr_stmt|;
name|pos
operator|--
expr_stmt|;
block|}
block|}
comment|// Reposition cursor to next free slot
name|cursor
operator|++
expr_stmt|;
if|if
condition|(
name|cursor
operator|>=
name|windowSize
condition|)
block|{
comment|// wrap around the buffer
name|cursor
operator|=
literal|0
expr_stmt|;
name|isWrapped
operator|=
literal|true
expr_stmt|;
block|}
comment|// clean out the end of the tail that we may overwrite if the
comment|// next iteration adds a new head
if|if
condition|(
name|isWrapped
condition|)
block|{
comment|// tokenPos is now positioned on tail - emit any valid
comment|// tokens we may about to overwrite in the next iteration
if|if
condition|(
name|priorStatesBuffer
index|[
name|cursor
index|]
operator|!=
literal|null
condition|)
block|{
name|recordLengthInfoState
argument_list|(
name|priorMaxNumSightings
argument_list|,
name|priorStatesBuffer
argument_list|,
name|cursor
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|// end loop reading all tokens from stream
comment|// Flush the buffered tokens
name|int
name|pos
init|=
name|isWrapped
condition|?
name|nextAfter
argument_list|(
name|cursor
argument_list|)
else|:
literal|0
decl_stmt|;
while|while
condition|(
name|pos
operator|!=
name|cursor
condition|)
block|{
name|recordLengthInfoState
argument_list|(
name|priorMaxNumSightings
argument_list|,
name|priorStatesBuffer
argument_list|,
name|pos
argument_list|)
expr_stmt|;
name|pos
operator|=
name|nextAfter
argument_list|(
name|pos
argument_list|)
expr_stmt|;
block|}
block|}
DECL|method|nextAfter
specifier|private
name|int
name|nextAfter
parameter_list|(
name|int
name|pos
parameter_list|)
block|{
name|pos
operator|++
expr_stmt|;
if|if
condition|(
name|pos
operator|>=
name|windowSize
condition|)
block|{
name|pos
operator|=
literal|0
expr_stmt|;
block|}
return|return
name|pos
return|;
block|}
DECL|method|recordLengthInfoState
specifier|private
name|void
name|recordLengthInfoState
parameter_list|(
name|short
index|[]
name|maxNumSightings
parameter_list|,
name|State
index|[]
name|tokenStates
parameter_list|,
name|int
name|cursor
parameter_list|)
block|{
if|if
condition|(
name|maxNumSightings
index|[
name|cursor
index|]
operator|>
literal|0
condition|)
block|{
comment|// We need to patch in the max sequence length we recorded at
comment|// this position into the token state
name|restoreState
argument_list|(
name|tokenStates
index|[
name|cursor
index|]
argument_list|)
expr_stmt|;
name|seqAtt
operator|.
name|setNumPriorUsesInASequence
argument_list|(
name|maxNumSightings
index|[
name|cursor
index|]
argument_list|)
expr_stmt|;
name|maxNumSightings
index|[
name|cursor
index|]
operator|=
literal|0
expr_stmt|;
comment|// record the patched state
name|tokenStates
index|[
name|cursor
index|]
operator|=
name|captureState
argument_list|()
expr_stmt|;
block|}
name|allTokens
operator|.
name|add
argument_list|(
name|tokenStates
index|[
name|cursor
index|]
argument_list|)
expr_stmt|;
block|}
block|}
block|}
end_class

end_unit

