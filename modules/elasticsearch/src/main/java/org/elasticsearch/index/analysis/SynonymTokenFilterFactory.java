begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to Elastic Search and Shay Banon under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership. Elastic Search licenses this  * file to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *    http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing,  * software distributed under the License is distributed on an  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY  * KIND, either express or implied.  See the License for the  * specific language governing permissions and limitations  * under the License.  */
end_comment

begin_package
DECL|package|org.elasticsearch.index.analysis
package|package
name|org
operator|.
name|elasticsearch
operator|.
name|index
operator|.
name|analysis
package|;
end_package

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|TokenStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|synonym
operator|.
name|SynonymFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|synonym
operator|.
name|SynonymMap
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|lucene
operator|.
name|analysis
operator|.
name|tokenattributes
operator|.
name|CharTermAttribute
import|;
end_import

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|ElasticSearchIllegalArgumentException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|common
operator|.
name|Strings
import|;
end_import

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|common
operator|.
name|inject
operator|.
name|Inject
import|;
end_import

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|common
operator|.
name|inject
operator|.
name|assistedinject
operator|.
name|Assisted
import|;
end_import

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|common
operator|.
name|io
operator|.
name|FastStringReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|common
operator|.
name|settings
operator|.
name|Settings
import|;
end_import

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|env
operator|.
name|Environment
import|;
end_import

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|index
operator|.
name|Index
import|;
end_import

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|index
operator|.
name|settings
operator|.
name|IndexSettings
import|;
end_import

begin_import
import|import
name|org
operator|.
name|elasticsearch
operator|.
name|indices
operator|.
name|analysis
operator|.
name|IndicesAnalysisService
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_class
annotation|@
name|AnalysisSettingsRequired
DECL|class|SynonymTokenFilterFactory
specifier|public
class|class
name|SynonymTokenFilterFactory
extends|extends
name|AbstractTokenFilterFactory
block|{
DECL|field|synonymMap
specifier|private
specifier|final
name|SynonymMap
name|synonymMap
decl_stmt|;
DECL|method|SynonymTokenFilterFactory
annotation|@
name|Inject
specifier|public
name|SynonymTokenFilterFactory
parameter_list|(
name|Index
name|index
parameter_list|,
annotation|@
name|IndexSettings
name|Settings
name|indexSettings
parameter_list|,
name|Environment
name|env
parameter_list|,
name|IndicesAnalysisService
name|indicesAnalysisService
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|TokenizerFactoryFactory
argument_list|>
name|tokenizerFactories
parameter_list|,
annotation|@
name|Assisted
name|String
name|name
parameter_list|,
annotation|@
name|Assisted
name|Settings
name|settings
parameter_list|)
block|{
name|super
argument_list|(
name|index
argument_list|,
name|indexSettings
argument_list|,
name|name
argument_list|,
name|settings
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|rules
init|=
name|Analysis
operator|.
name|getWordList
argument_list|(
name|env
argument_list|,
name|settings
argument_list|,
literal|"synonyms"
argument_list|)
decl_stmt|;
if|if
condition|(
name|rules
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|ElasticSearchIllegalArgumentException
argument_list|(
literal|"synonym requires either `synonyms` or `synonyms_path` to be configured"
argument_list|)
throw|;
block|}
name|boolean
name|ignoreCase
init|=
name|settings
operator|.
name|getAsBoolean
argument_list|(
literal|"ignore_case"
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|boolean
name|expand
init|=
name|settings
operator|.
name|getAsBoolean
argument_list|(
literal|"expand"
argument_list|,
literal|true
argument_list|)
decl_stmt|;
name|String
name|tokenizerName
init|=
name|settings
operator|.
name|get
argument_list|(
literal|"tokenizer"
argument_list|,
literal|"whitespace"
argument_list|)
decl_stmt|;
name|TokenizerFactoryFactory
name|tokenizerFactoryFactory
init|=
name|tokenizerFactories
operator|.
name|get
argument_list|(
name|tokenizerName
argument_list|)
decl_stmt|;
if|if
condition|(
name|tokenizerFactoryFactory
operator|==
literal|null
condition|)
block|{
name|tokenizerFactoryFactory
operator|=
name|indicesAnalysisService
operator|.
name|tokenizerFactoryFactory
argument_list|(
name|tokenizerName
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|tokenizerFactoryFactory
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|ElasticSearchIllegalArgumentException
argument_list|(
literal|"failed to fine tokenizer ["
operator|+
name|tokenizerName
operator|+
literal|"] for synonym token filter"
argument_list|)
throw|;
block|}
name|TokenizerFactory
name|tokenizerFactory
init|=
name|tokenizerFactoryFactory
operator|.
name|create
argument_list|(
name|tokenizerName
argument_list|,
name|settings
argument_list|)
decl_stmt|;
name|synonymMap
operator|=
operator|new
name|SynonymMap
argument_list|(
name|ignoreCase
argument_list|)
expr_stmt|;
name|parseRules
argument_list|(
name|rules
argument_list|,
name|synonymMap
argument_list|,
literal|"=>"
argument_list|,
literal|","
argument_list|,
name|expand
argument_list|,
name|tokenizerFactory
argument_list|)
expr_stmt|;
block|}
DECL|method|create
annotation|@
name|Override
specifier|public
name|TokenStream
name|create
parameter_list|(
name|TokenStream
name|tokenStream
parameter_list|)
block|{
return|return
operator|new
name|SynonymFilter
argument_list|(
name|tokenStream
argument_list|,
name|synonymMap
argument_list|)
return|;
block|}
DECL|method|parseRules
specifier|static
name|void
name|parseRules
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|rules
parameter_list|,
name|SynonymMap
name|map
parameter_list|,
name|String
name|mappingSep
parameter_list|,
name|String
name|synSep
parameter_list|,
name|boolean
name|expansion
parameter_list|,
name|TokenizerFactory
name|tokFactory
parameter_list|)
block|{
name|int
name|count
init|=
literal|0
decl_stmt|;
for|for
control|(
name|String
name|rule
range|:
name|rules
control|)
block|{
comment|// To use regexes, we need an expression that specifies an odd number of chars.
comment|// This can't really be done with string.split(), and since we need to
comment|// do unescaping at some point anyway, we wouldn't be saving any effort
comment|// by using regexes.
name|List
argument_list|<
name|String
argument_list|>
name|mapping
init|=
name|Strings
operator|.
name|splitSmart
argument_list|(
name|rule
argument_list|,
name|mappingSep
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|source
decl_stmt|;
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|target
decl_stmt|;
if|if
condition|(
name|mapping
operator|.
name|size
argument_list|()
operator|>
literal|2
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Invalid Synonym Rule:"
operator|+
name|rule
argument_list|)
throw|;
block|}
elseif|else
if|if
condition|(
name|mapping
operator|.
name|size
argument_list|()
operator|==
literal|2
condition|)
block|{
name|source
operator|=
name|getSynList
argument_list|(
name|mapping
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|synSep
argument_list|,
name|tokFactory
argument_list|)
expr_stmt|;
name|target
operator|=
name|getSynList
argument_list|(
name|mapping
operator|.
name|get
argument_list|(
literal|1
argument_list|)
argument_list|,
name|synSep
argument_list|,
name|tokFactory
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|source
operator|=
name|getSynList
argument_list|(
name|mapping
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|synSep
argument_list|,
name|tokFactory
argument_list|)
expr_stmt|;
if|if
condition|(
name|expansion
condition|)
block|{
comment|// expand to all arguments
name|target
operator|=
name|source
expr_stmt|;
block|}
else|else
block|{
comment|// reduce to first argument
name|target
operator|=
operator|new
name|ArrayList
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|(
literal|1
argument_list|)
expr_stmt|;
name|target
operator|.
name|add
argument_list|(
name|source
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
name|boolean
name|includeOrig
init|=
literal|false
decl_stmt|;
for|for
control|(
name|List
argument_list|<
name|String
argument_list|>
name|fromToks
range|:
name|source
control|)
block|{
name|count
operator|++
expr_stmt|;
for|for
control|(
name|List
argument_list|<
name|String
argument_list|>
name|toToks
range|:
name|target
control|)
block|{
name|map
operator|.
name|add
argument_list|(
name|fromToks
argument_list|,
name|SynonymMap
operator|.
name|makeTokens
argument_list|(
name|toToks
argument_list|)
argument_list|,
name|includeOrig
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|// a , b c , d e f => [[a],[b,c],[d,e,f]]
DECL|method|getSynList
specifier|private
specifier|static
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|getSynList
parameter_list|(
name|String
name|str
parameter_list|,
name|String
name|separator
parameter_list|,
name|TokenizerFactory
name|tokFactory
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|strList
init|=
name|Strings
operator|.
name|splitSmart
argument_list|(
name|str
argument_list|,
name|separator
argument_list|,
literal|false
argument_list|)
decl_stmt|;
comment|// now split on whitespace to get a list of token strings
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|synList
init|=
operator|new
name|ArrayList
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|String
name|toks
range|:
name|strList
control|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|tokList
init|=
name|tokFactory
operator|==
literal|null
condition|?
name|Strings
operator|.
name|splitWS
argument_list|(
name|toks
argument_list|,
literal|true
argument_list|)
else|:
name|splitByTokenizer
argument_list|(
name|toks
argument_list|,
name|tokFactory
argument_list|)
decl_stmt|;
name|synList
operator|.
name|add
argument_list|(
name|tokList
argument_list|)
expr_stmt|;
block|}
return|return
name|synList
return|;
block|}
DECL|method|splitByTokenizer
specifier|private
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|splitByTokenizer
parameter_list|(
name|String
name|source
parameter_list|,
name|TokenizerFactory
name|tokFactory
parameter_list|)
block|{
name|TokenStream
name|ts
init|=
name|tokFactory
operator|.
name|create
argument_list|(
operator|new
name|FastStringReader
argument_list|(
name|source
argument_list|)
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|tokList
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
name|CharTermAttribute
name|termAtt
init|=
name|ts
operator|.
name|addAttribute
argument_list|(
name|CharTermAttribute
operator|.
name|class
argument_list|)
decl_stmt|;
while|while
condition|(
name|ts
operator|.
name|incrementToken
argument_list|()
condition|)
block|{
if|if
condition|(
name|termAtt
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
name|tokList
operator|.
name|add
argument_list|(
name|termAtt
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|tokList
return|;
block|}
block|}
end_class

end_unit

